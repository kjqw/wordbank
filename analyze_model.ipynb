{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(680, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 680),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu, log_var = x.chunk(2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(data_loader):\n",
    "        data = data.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    average_loss = train_loss / len(data_loader.dataset)\n",
    "    print(f\"Average loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "with open(\"tmp/data_id_dict.pkl\", \"rb\") as f:\n",
    "    data_id_dict = pickle.load(f)\n",
    "with open(\"tmp/child_id_dict.pkl\", \"rb\") as f:\n",
    "    child_id_dict = pickle.load(f)\n",
    "with open(\"tmp/word_dict.pkl\", \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "with open(\"tmp/category_dict.pkl\", \"rb\") as f:\n",
    "    category_dict = pickle.load(f)\n",
    "\n",
    "tensor_data = torch.tensor(data.astype(np.float32))\n",
    "dataset = TensorDataset(tensor_data)\n",
    "data_loader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "model = VAE().to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"tmp/best_model.pth\"))\n",
    "# model.load_state_dict(torch.load(\"tmp/model_state_dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語データを潜在変数に変換\n",
    "def x_to_z(model: VAE, xs: np.ndarray) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xs = torch.tensor(xs.astype(np.float32)).cuda()\n",
    "        zs = model.encoder(xs)\n",
    "        mu, log_var = zs.chunk(2, dim=1)\n",
    "        z_points = mu.cpu()\n",
    "        z_points = np.array(z_points)\n",
    "        return z_points\n",
    "\n",
    "\n",
    "# 潜在変数を単語データに変換\n",
    "def z_to_x(model: VAE, z: np.ndarray) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.tensor(z.astype(np.float32)).cuda()\n",
    "        xs = model.decoder(z)\n",
    "        xs = np.array(xs.cpu())\n",
    "        return xs\n",
    "\n",
    "\n",
    "def get_vocabulary(xs: np.ndarray) -> np.ndarray:\n",
    "    return np.sum(xs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = np.array([[0.5 for i in range(680)]])\n",
    "data_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_origin(model, ax):\n",
    "    all_0s = np.zeros((1, 680))\n",
    "    z0 = x_to_z(model, all_0s)\n",
    "    all_1s = np.ones((1, 680))\n",
    "    z1 = x_to_z(model, all_1s)\n",
    "    ax.scatter(z0[:, 0], z0[:, 1], color=\"blue\", label=\"all 0s\", marker=\"x\")\n",
    "    ax.scatter(z1[:, 0], z1[:, 1], color=\"red\", label=\"all 1s\", marker=\"*\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_x(model, xs, ax):\n",
    "    zs = x_to_z(model, xs)\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], s=0.2)\n",
    "\n",
    "    plot_origin(model, ax)\n",
    "    ax.set_xlabel(r\"$z_{1}$\")\n",
    "    ax.set_ylabel(r\"$z_{2}$\")\n",
    "\n",
    "\n",
    "def plot_x_with_vocabulary(model, data_ids, ax):\n",
    "    xs = data[data_ids]\n",
    "    vocabulary = get_vocabulary(xs)\n",
    "    zs = x_to_z(model, xs)\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], c=vocabulary, cmap=\"turbo\", s=0.2)\n",
    "\n",
    "    plot_origin(model, ax)\n",
    "    ax.set_xlabel(r\"$z_{1}$\")\n",
    "    ax.set_ylabel(r\"$z_{2}$\")\n",
    "\n",
    "\n",
    "def plot_x_with_age(model, data_ids, ax):\n",
    "    xs = data[data_ids]\n",
    "    ages = np.array([data_id_dict[i][1] for i in data_ids])\n",
    "    zs = x_to_z(model, xs)\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], c=ages, cmap=\"turbo\", s=0.2)\n",
    "\n",
    "    plot_origin(model, ax)\n",
    "    ax.set_xlabel(r\"$z_{1}$\")\n",
    "    ax.set_ylabel(r\"$z_{2}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = {}\n",
    "figs[\"age\"] = plt.subplots()\n",
    "figs[\"vocabulary\"] = plt.subplots()\n",
    "\n",
    "data_ids = list(data_id_dict.keys())\n",
    "plot_x_with_age(model, data_ids, figs[\"age\"][1])\n",
    "plot_x_with_vocabulary(model, data_ids, figs[\"vocabulary\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 潜在空間の格子点\n",
    "import itertools\n",
    "\n",
    "\n",
    "def make_lattice_points(\n",
    "    z1_start: float, z1_end: float, z2_start: float, z2_end: float, spacing: float\n",
    ") -> np.float32:\n",
    "    z1 = np.arange(z1_start, z1_end + spacing, spacing)\n",
    "    z2 = np.arange(z2_start, z2_end + spacing, spacing)\n",
    "\n",
    "    zs = np.array(list(itertools.product(z1, z2)), dtype=np.float32)\n",
    "    return zs, (len(z1), len(z2))\n",
    "\n",
    "\n",
    "def plot_vocabulary(model, z1_start, z1_end, z2_start, z2_end, spacing, fig, ax):\n",
    "    zs, size = make_lattice_points(z1_start, z1_end, z2_start, z2_end, spacing)\n",
    "    data = z_to_x(model, zs)\n",
    "    vocabulary = get_vocabulary(data).reshape(size[::-1], order=\"F\")\n",
    "    im = ax.imshow(\n",
    "        vocabulary,\n",
    "        extent=[z1_start, z1_end, z2_start, z2_end],\n",
    "        cmap=\"turbo\",\n",
    "        origin=\"lower\",\n",
    "    )\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set_xlabel(r\"$z_{1}$\")\n",
    "    ax.set_ylabel(r\"$z_{2}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_start, z1_end = -4, 5\n",
    "z2_start, z2_end = -3, 3\n",
    "spacing = 0.1\n",
    "\n",
    "zs, size = make_lattice_points(z1_start, z1_end, z2_start, z2_end, spacing)\n",
    "figs[\"latent_vocabulary\"] = plt.subplots()\n",
    "plot_vocabulary(\n",
    "    model,\n",
    "    z1_start,\n",
    "    z1_end,\n",
    "    z2_start,\n",
    "    z2_end,\n",
    "    spacing,\n",
    "    figs[\"latent_vocabulary\"][0],\n",
    "    figs[\"latent_vocabulary\"][1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "figs[\"tmp\"] = plt.subplots()\n",
    "data_ids = []\n",
    "for i, v in child_id_dict.items():\n",
    "    if len(v) >= 2:\n",
    "        data_ids.append([j[0] for j in v])\n",
    "# print(data_ids)\n",
    "n = 20\n",
    "datas = random.sample(data_ids, n)\n",
    "for i in datas:\n",
    "    zs = x_to_z(model, data[i])\n",
    "    z1, z2 = zs[:, 0], zs[:, 1]\n",
    "    plt.arrow(\n",
    "        z1[0],\n",
    "        z2[0],\n",
    "        z1[1] - z1[0],\n",
    "        z2[1] - z2[0],\n",
    "        head_width=0.1,\n",
    "        head_length=0.1,\n",
    "        fc=\"k\",\n",
    "        ec=\"k\",\n",
    "    )\n",
    "# plot_vocabulary(model, z1_start, z1_end, z2_start, z2_end, spacing)\n",
    "plot_x(model, data, figs[\"tmp\"][1])\n",
    "print(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[\"age\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[\"vocabulary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[\"latent_vocabulary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[\"tmp\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
