{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.figure import Figure\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(680, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 680),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu, log_var = x.chunk(2, dim=1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data,) in enumerate(data_loader):\n",
    "        data = data.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    average_loss = train_loss / len(data_loader.dataset)\n",
    "    print(f\"Average loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "with open(\"tmp/data_id_dict.pkl\", \"rb\") as f:\n",
    "    data_id_dict = pickle.load(f)\n",
    "with open(\"tmp/child_id_dict.pkl\", \"rb\") as f:\n",
    "    child_id_dict = pickle.load(f)\n",
    "with open(\"tmp/word_dict.pkl\", \"rb\") as f:\n",
    "    word_dict = pickle.load(f)\n",
    "with open(\"tmp/category_dict.pkl\", \"rb\") as f:\n",
    "    category_dict = pickle.load(f)\n",
    "\n",
    "tensor_data = torch.tensor(data.astype(np.float32))\n",
    "dataset = TensorDataset(tensor_data)\n",
    "data_loader = DataLoader(dataset, batch_size=64)\n",
    "\n",
    "model = VAE().to(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"tmp/best_model.pth\"))\n",
    "# model.load_state_dict(torch.load(\"tmp/model_state_dict.pth\"))\n",
    "\n",
    "word_count = len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語データを潜在変数に変換\n",
    "def x_to_z(model: VAE, xs: np.ndarray) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xs = torch.tensor(xs.astype(np.float32)).cuda()\n",
    "        zs = model.encoder(xs)\n",
    "        mu, log_var = zs.chunk(2, dim=1)\n",
    "        z_points = mu.cpu()\n",
    "        z_points = np.array(z_points)\n",
    "        return z_points\n",
    "\n",
    "\n",
    "# 潜在変数を単語データに変換\n",
    "def z_to_x(model: VAE, z: np.ndarray) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.tensor(z.astype(np.float32)).cuda()\n",
    "        xs = model.decoder(z)\n",
    "        xs = np.array(xs.cpu())\n",
    "        return xs\n",
    "\n",
    "\n",
    "def category_to_num(categories: list[str]) -> list[int]:\n",
    "    nums = []\n",
    "    if categories == [\"all\"]:\n",
    "        return list(range(word_count))\n",
    "    for category in categories:\n",
    "        nums.extend([i[0] for i in category_dict[category]])\n",
    "    return nums\n",
    "\n",
    "\n",
    "def get_vocabulary(xs: np.ndarray, categories: list[str] = [\"all\"]) -> np.ndarray:\n",
    "    nums = category_to_num(categories)\n",
    "    return np.sum(xs[..., nums], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_origin(model: VAE, ax: Axes) -> None:\n",
    "    all_0s = np.zeros((1, 680))\n",
    "    z0 = x_to_z(model, all_0s)\n",
    "    all_1s = np.ones((1, 680))\n",
    "    z1 = x_to_z(model, all_1s)\n",
    "    ax.scatter(z0[:, 0], z0[:, 1], color=\"blue\", label=\"all 0s\", marker=\"x\")\n",
    "    ax.scatter(z1[:, 0], z1[:, 1], color=\"red\", label=\"all 1s\", marker=\"*\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def set_labels(ax: Axes, title: str = \"\") -> None:\n",
    "    ax.set_xlabel(r\"$z_{1}$\")\n",
    "    ax.set_ylabel(r\"$z_{2}$\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_x(model: VAE, xs: np.ndarray, ax: Axes) -> None:\n",
    "    zs = x_to_z(model, xs)\n",
    "    ax.scatter(zs[:, 0], zs[:, 1], s=0.2)\n",
    "\n",
    "    plot_origin(model, ax)\n",
    "\n",
    "\n",
    "def plot_x_with_age(model: VAE, data_ids: list[int], fig: Figure, ax: Axes) -> None:\n",
    "    xs = data[data_ids]\n",
    "    ages = np.array([data_id_dict[i][1] for i in data_ids])\n",
    "    zs = x_to_z(model, xs)\n",
    "    scatter = ax.scatter(zs[:, 0], zs[:, 1], c=ages, cmap=\"turbo\", s=0.2)\n",
    "    cbar = fig.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(\"age\")\n",
    "\n",
    "    plot_origin(model, ax)\n",
    "    set_labels(ax, \"age\")\n",
    "\n",
    "\n",
    "def plot_x_with_vocabulary(\n",
    "    model: VAE,\n",
    "    data_ids: list[int],\n",
    "    fig: Figure,\n",
    "    ax: Axes,\n",
    "    categories: list[str] = [\"all\"],\n",
    ") -> None:\n",
    "    xs = data[data_ids]\n",
    "    vocabulary = get_vocabulary(xs, categories)\n",
    "    zs = x_to_z(model, xs)\n",
    "    scatter = ax.scatter(zs[:, 0], zs[:, 1], c=vocabulary, cmap=\"turbo\", s=0.2)\n",
    "    cbar = fig.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label(\"vocabulary\")\n",
    "\n",
    "    plot_origin(model, ax)\n",
    "    set_labels(ax, \", \".join(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in category_dict.items():\n",
    "    print(key, len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = {}\n",
    "figs[\"age\"] = plt.subplots()\n",
    "figs[\"vocabulary\"] = plt.subplots()\n",
    "\n",
    "data_ids = list(data_id_dict.keys())\n",
    "plot_x_with_age(model, data_ids, *figs[\"age\"])\n",
    "plot_x_with_vocabulary(model, data_ids, *figs[\"vocabulary\"], [\"locations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 潜在空間の格子点\n",
    "import itertools\n",
    "\n",
    "\n",
    "def make_lattice_points(\n",
    "    z1_start: float,\n",
    "    z1_end: float,\n",
    "    z2_start: float,\n",
    "    z2_end: float,\n",
    "    spacing: float,\n",
    ") -> np.float32:\n",
    "    z1 = np.arange(z1_start, z1_end + spacing, spacing)\n",
    "    z2 = np.arange(z2_start, z2_end + spacing, spacing)\n",
    "\n",
    "    return np.meshgrid(z1, z2)\n",
    "\n",
    "\n",
    "def plot_vocabulary(\n",
    "    model: VAE,\n",
    "    z_mashgrid,\n",
    "    fig: Figure,\n",
    "    ax: Axes,\n",
    "    categories: list[str] = [\"all\"],\n",
    ") -> None:\n",
    "    z1, z2 = z_mashgrid\n",
    "    zs = np.dstack((z1, z2))\n",
    "    xs = z_to_x(model, zs)\n",
    "    vocabulary = get_vocabulary(xs, categories)\n",
    "    cmap = ax.pcolormesh(z1, z2, vocabulary, cmap=\"Greys_r\")\n",
    "    cbar = fig.colorbar(cmap, ax=ax)\n",
    "    cbar.set_label(\"vocabulary\")\n",
    "    set_labels(ax, \", \".join(categories))\n",
    "\n",
    "\n",
    "def plot_arrow(model: VAE, data_ids: list[int], ax: Axes) -> None:\n",
    "    xs = data[data_ids]\n",
    "    zs = x_to_z(model, xs)\n",
    "    for z1, z2 in zip(zs[0:, :], zs[1:, :]):\n",
    "        ax.annotate(\n",
    "            \"\",\n",
    "            xy=z2,\n",
    "            xytext=z1,\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"black\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_start, z1_end = -6, 7\n",
    "z2_start, z2_end = -3, 3\n",
    "spacing = 0.1\n",
    "\n",
    "z_meshgrid = make_lattice_points(z1_start, z1_end, z2_start, z2_end, spacing)\n",
    "# print(np.dstack((z1, z2)).shape)\n",
    "for category in category_dict.keys():\n",
    "    figs[category] = plt.subplots()\n",
    "    plot_vocabulary(model, z_meshgrid, *figs[category], [category])\n",
    "    plot_x(model, data, figs[category][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "figs[\"arrow\"] = plt.subplots()\n",
    "plot_x(model, data, figs[\"arrow\"][1])\n",
    "data_ids = []\n",
    "for i, v in child_id_dict.items():\n",
    "    if len(v) >= 2:\n",
    "        data_ids.append([j[0] for j in v])\n",
    "print(len(data_ids))\n",
    "n = 1\n",
    "datas = random.sample(data_ids, n)\n",
    "child_id = data_id_dict[datas[0][0]][0]\n",
    "print(child_id_dict[child_id])\n",
    "for i in datas:\n",
    "    plot_arrow(model, i, figs[\"arrow\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "\n",
    "def make_circle(r: float, fig: Figure, ax: Axes) -> None:\n",
    "    all_0s = np.zeros((1, 680))\n",
    "    z0 = x_to_z(model, all_0s).flatten()\n",
    "    all_1s = np.ones((1, 680))\n",
    "    z1 = x_to_z(model, all_1s).flatten()\n",
    "    mid = (z0 + z1) / 2\n",
    "    # 2点間の距離\n",
    "    d = np.linalg.norm(z1 - z0)\n",
    "\n",
    "    # 2点間の中心からの距離\n",
    "    h = np.sqrt(r**2 - (d / 2) ** 2)\n",
    "\n",
    "    # 中心点を見つけるための単位ベクトルの計算\n",
    "    vec = z1 - z0\n",
    "    vec_perp = np.array([-vec[1], vec[0]])\n",
    "    unit_vec_perp = vec_perp / np.linalg.norm(vec_perp)\n",
    "\n",
    "    # 中心点Cの計算\n",
    "    C1 = mid + h * unit_vec_perp\n",
    "    C2 = mid - h * unit_vec_perp\n",
    "\n",
    "    # 円を描く\n",
    "    circle1 = Circle(C1, r, fill=False, color=\"black\")\n",
    "    circle2 = Circle(C2, r, fill=False, color=\"black\")\n",
    "    ax.add_patch(circle1)\n",
    "    ax.add_patch(circle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[\"tmp\"] = plt.subplots()\n",
    "plot_x(model, data, figs[\"tmp\"][1])\n",
    "make_circle(2.15, *figs[\"tmp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[\"tmp\"] = plt.subplots()\n",
    "plot_vocabulary(model, z_meshgrid, *figs[\"tmp\"], [\"all\"])\n",
    "plot_x(model, data, figs[\"tmp\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
